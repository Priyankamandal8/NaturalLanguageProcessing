{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab04Solved_pman7719.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyankamandal8/NaturalLanguageProcessing/blob/master/Lab04Solved_pman7719.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgAANQLNyE3p",
        "colab_type": "text"
      },
      "source": [
        "# Lab04\n",
        "\n",
        "In this lab 4, we learn the Recurrent Neural Networks and Sequence Modelling\n",
        "\n",
        "\n",
        "*   Recurrent Neural Networks\n",
        "*   Sequence Modelling (Seq2Seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-316qjIt-zS",
        "colab_type": "text"
      },
      "source": [
        "# Exercise (Text classification using LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwj7GtQPP6M7",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, you are going to implement a LSTM model to do the text classification problem. Please notice that we have already done the preprocessing and embedding part of the dataset. You can only focus on the Model part.\n",
        "\n",
        "**Sequence Modelling**\n",
        "\n",
        "![alt text](https://usydnlpgroup.files.wordpress.com/2020/03/lstm_textclassification-e1584855309361.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6E9-9EqTNns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "#If you enable GPU here, device will be cuda, otherwise it will be cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-hlDQBqsV1L",
        "colab_type": "text"
      },
      "source": [
        "### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvwyvmfDegwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1ORrHW9moXLcWwg8WY9o-Ulq8X9BAiD1P'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.pkl')  \n",
        "\n",
        "id = '1eb4gtE8XlN3TcZqzwS18Ik-H7MFAeW4z'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('label.pkl')  \n",
        "\n",
        "import pickle\n",
        "input_embeddings = pickle.load(open(\"train.pkl\",\"rb\"))\n",
        "label = pickle.load(open(\"label.pkl\",\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4iYaoduQvA9",
        "colab_type": "text"
      },
      "source": [
        "### Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmkbXPApluQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing dataset using scikit-learn\n",
        "# For more details, you can refer to: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_embeddings, test_embeddings, train_label, test_label = train_test_split(input_embeddings,label,test_size = 0.2, random_state=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrAL53HKufcG",
        "colab_type": "text"
      },
      "source": [
        "## Generate batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4vtcmaZufk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(input_embeddings,label, batch_size):\n",
        "    idx = np.random.randint(input_embeddings.shape[0], size=batch_size)\n",
        "    return input_embeddings[idx,:,:],label[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_W08vIPv7L7",
        "colab_type": "text"
      },
      "source": [
        "## Model (please complete the following sections)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3rPdMUDQFOU",
        "colab_type": "text"
      },
      "source": [
        "**NOTE**: By updating hyperparameters, you should achieve **at least 0.4** for testset \"weighted avg\" f1. (There will be randomness in the training process, so tutors would run your code several times and there should be at least one of the output reaching 0.4)\n",
        "\n",
        "***What is F1?***\n",
        "\n",
        "![alt text](https://1.bp.blogspot.com/-nkFFqViboVM/XWwaQ5x1YpI/AAAAAAAAAP8/XzTH9hfJSfswcRjxSeQFEU6-yKQCwc0EQCLcBGAs/s640/main-qimg-447d6cdb02d2cc097ff1e6083a6bdc37.png)\n",
        "![alt text](https://i.stack.imgur.com/U0hjG.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4o1hzIzuDvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True)\n",
        "        self.linear = nn.Linear(n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):        \n",
        "        # Please complete the code for forwardpropagation\n",
        "        # lstm layer\n",
        "        x,_ = self.lstm(x)\n",
        "\n",
        "        # linear layer\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        \n",
        "        # softmax layer\n",
        "        x=F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mklFWbwhrsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "# Please assign values to these variables by using other variables (instead of hard code)\n",
        "seq_length = len(train_embeddings[0]) #No. of sequence input from train_embeddings\n",
        "n_input = len(train_embeddings[0][0])  #No of input items from the train_embeddings\n",
        "n_class = len(list(set(test_label)))  #No of unique elements from label set for the output\n",
        "\n",
        "#Please decide the hyperparameters here by yourself\n",
        "n_hidden = 128\n",
        "batch_size = 128\n",
        "total_epoch = 1000\n",
        "learning_rate = 0.05\n",
        "shown_interval = 100\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNKfUFdRnQJo",
        "colab_type": "code",
        "outputId": "1265c85b-d4d2-4167-a8e2-66161c92e3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "net = Net().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Please find which optimizer provide higher f1\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "\n",
        "    input_batch, target_batch = generate_batch(train_embeddings,train_label, batch_size)\n",
        "    input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "    target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "    net.train()\n",
        "    outputs = net(input_batch_torch) \n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % shown_interval == shown_interval-1:\n",
        "        net.eval()\n",
        "        outputs = net(input_batch_torch) \n",
        "        train_loss = criterion(outputs, target_batch_torch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "\n",
        "        print('Epoch: %d, train loss: %.5f, train_acc: %.4f'%(epoch + 1, train_loss.item(), train_acc))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "net.eval()\n",
        "outputs = net(torch.from_numpy(test_embeddings).float().to(device)) \n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_label, predicted.cpu().numpy(),digits=4))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100, train loss: 0.99796, train_acc: 0.4766\n",
            "Epoch: 200, train loss: 0.47127, train_acc: 0.7734\n",
            "Epoch: 300, train loss: 0.37888, train_acc: 0.8359\n",
            "Epoch: 400, train loss: 0.34990, train_acc: 0.8359\n",
            "Epoch: 500, train loss: 0.18187, train_acc: 0.9141\n",
            "Epoch: 600, train loss: 0.08888, train_acc: 0.9688\n",
            "Epoch: 700, train loss: 0.06564, train_acc: 0.9688\n",
            "Epoch: 800, train loss: 0.01522, train_acc: 1.0000\n",
            "Epoch: 900, train loss: 0.05386, train_acc: 0.9844\n",
            "Epoch: 1000, train loss: 0.03382, train_acc: 0.9844\n",
            "Finished Training\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7115    0.7957    0.7513        93\n",
            "           1     0.9316    0.8934    0.9121       122\n",
            "           2     0.8812    0.8165    0.8476       109\n",
            "           3     0.8000    0.8125    0.8062       128\n",
            "\n",
            "    accuracy                         0.8319       452\n",
            "   macro avg     0.8311    0.8295    0.8293       452\n",
            "weighted avg     0.8369    0.8319    0.8335       452\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LJJmS7udQc4q",
        "colab": {}
      },
      "source": [
        "#The following is the sample output \n",
        "#As mentioned in the previous labs, it is impossible to get the same result (randomness in the training process)."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}